{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install vaderSentiment\n",
    "# !pip install textstat\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vaderSentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e38fb35cb972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpanda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vaderSentiment'"
     ]
    }
   ],
   "source": [
    "import pandas as panda\n",
    "import time\n",
    "import vaderSentiment\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn\n",
    "from textstat.textstat import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!\"@__BrighterDays: I can not just sit up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!&amp;#8220;@selfiequeenbri: cause I'm tired of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" &amp;amp; you might not get ya bitch back &amp;amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>\" @rhythmixx_ :hobbies include: fighting Maria...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "5           5      3            1                   2        0      1   \n",
       "6           6      3            0                   3        0      1   \n",
       "7           7      3            0                   3        0      1   \n",
       "8           8      3            0                   3        0      1   \n",
       "9           9      3            1                   2        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "5  !!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just...  \n",
       "6  !!!!!!\"@__BrighterDays: I can not just sit up ...  \n",
       "7  !!!!&#8220;@selfiequeenbri: cause I'm tired of...  \n",
       "8  \" &amp; you might not get ya bitch back &amp; ...  \n",
       "9  \" @rhythmixx_ :hobbies include: fighting Maria...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = panda.read_csv(\"HateSpeechData.csv\")\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28025aec688>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['class'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The above histogram shows that most of the tweets are considered to be offensive words by the CF coders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting only the tweets from the csv file into a variable name tweet\n",
    "tweet=dataset.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ae46be992576>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m## 4. Stemming\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "## 1. Removal of punctuation and capitlization\n",
    "## 2. Tokenizing\n",
    "## 3. Removal of stopwords\n",
    "## 4. Stemming\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "#extending the stopwords to include other words used in twitter such as retweet(rt) etc.\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess(tweet):  \n",
    "    \n",
    "    # removal of extra spaces\n",
    "    regex_pat = re.compile(r'\\s+')\n",
    "    tweet_space = tweet.str.replace(regex_pat, ' ')\n",
    "\n",
    "    # removal of @name[mention]\n",
    "    regex_pat = re.compile(r'@[\\w\\-]+')\n",
    "    tweet_name = tweet_space.str.replace(regex_pat, '')\n",
    "\n",
    "    # removal of links[https://abc.com]\n",
    "    giant_url_regex =  re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "            '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    tweets = tweet_name.str.replace(giant_url_regex, '')\n",
    "    \n",
    "    # removal of punctuations and numbers\n",
    "    punc_remove = tweets.str.replace(\"[^a-zA-Z]\", \" \")\n",
    "    # removal of capitalization\n",
    "    tweet_lower = punc_remove.str.lower()\n",
    "    \n",
    "    # tokenizing\n",
    "    tokenized_tweet = tweet_lower.apply(lambda x: x.split())\n",
    "    \n",
    "    # removal of stopwords\n",
    "    tokenized_tweet=  tokenized_tweet.apply(lambda x: [item for item in x if item not in stopwords])\n",
    "    \n",
    "    # stemming of the tweets\n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) \n",
    "    \n",
    "    for i in range(len(tokenized_tweet)):\n",
    "        tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
    "        tweets_p= tokenized_tweet\n",
    "    \n",
    "    return tweets_p\n",
    "\n",
    "processed_tweets = preprocess(tweet)   \n",
    "\n",
    "dataset['processed_tweets'] = processed_tweets\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing which of the word is most commonly used in the twitter dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "all_words = ' '.join([text for text in dataset['processed_tweets'] ])\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing which of the word is most commonly used for hatred speech\n",
    "hatred_words = ' '.join([text for text in dataset['processed_tweets'][dataset['class'] == 1]])\n",
    "wordcloud = WordCloud(width=800, height=500,\n",
    "random_state=21, max_font_size=110).generate(hatred_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing which of the word is most commonly used for offensive speech\n",
    "offensive_words = ' '.join([text for text in dataset['processed_tweets'][dataset['class'] == 2]])\n",
    "wordcloud = WordCloud(width=800, height=500,\n",
    "random_state=21, max_font_size=110).generate(offensive_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Features\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2),max_df=0.75, min_df=1, max_features=10000)\n",
    "# bigram feature matrix\n",
    "bigram = bigram_vectorizer.fit_transform(processed_tweets).toarray()\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Features\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),max_df=0.75, min_df=5, max_features=10000)\n",
    "\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset['processed_tweets'] )\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3e1a434c04d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Using Bigram Features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpanda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_train_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bigram' is not defined"
     ]
    }
   ],
   "source": [
    "# Using Bigram Features\n",
    "X = panda.DataFrame(bigram)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
    "\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced',penalty=\"l2\", C=0.01).fit(X_train_bow,y_train)\n",
    "y_preds = model.predict(X_test_bow)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "\n",
    "print(\"Accuracy Score:\" , accuracy_score(y_test,y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model Using TFIDF without additional features\n",
    "\n",
    "X = tfidf\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
    "\n",
    "\n",
    "model = LogisticRegression(penalty='l2',  max_iter=1000).fit(X_train_tfidf,y_train)\n",
    "y_preds = model.predict(X_test_tfidf)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "\n",
    "print(\"Accuracy Score:\" , accuracy_score(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test cells begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()\n",
    "\n",
    "sent_dict = sia.polarity_scores(\"You are such a piece of shit man!\")\n",
    "sent_dict\n",
    "# count_tags(\"Hello there https://google.com @google_USA is a total #wasteOfTime\")\n",
    "# sentiment_analysis_array(tweet[:5])\n",
    "# tfidf_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(18, 36, 2).reshape(3, 3).astype('int')\n",
    "b = np.arange(1, 10, 1).reshape(3, 3).astype('int')\n",
    "np.concatenate((a, b), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test cells end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis\n",
    "sentiment_analyzer = VS()\n",
    "def count_tags(tweet_c):  \n",
    "    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', tweet_c)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def sentiment_analysis(tweet):   \n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)    \n",
    "    twitter_objs = count_tags(tweet)\n",
    "    features = [sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],twitter_objs[0], twitter_objs[1],\n",
    "                twitter_objs[2]]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def sentiment_analysis_array(tweets):\n",
    "    features=[]\n",
    "    for t in tweets:\n",
    "        features.append(sentiment_analysis(t))\n",
    "    return np.array(features)\n",
    "\n",
    "final_features = sentiment_analysis_array(tweet)\n",
    "#final_features\n",
    "\n",
    "new_features = panda.DataFrame({'Neg':final_features[:,0],'Pos':final_features[:,1],'Neu':final_features[:,2],'Compound':final_features[:,3],\n",
    "                            'url_tag':final_features[:,4],'mention_tag':final_features[:,5],'hash_tag':final_features[:,6]})\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_a = tfidf.toarray()\n",
    "modelling_features = np.concatenate([tfidf_a,final_features],axis=1)\n",
    "print(f\"modelling_features.shape: {modelling_features.shape}\\ntfidf.shape: {tfidf_a.shape}\\nfinal_features.shape: {final_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model Using TFIDF with some features from sentiment analysis\n",
    "\n",
    "X = panda.DataFrame(modelling_features)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
    "\n",
    "model_acc = LogisticRegression(max_iter=1000).fit(X_train_bow,y_train)\n",
    "y_preds = model_acc.predict(X_test_bow)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "\n",
    "print(\"Accuracy Score:\" , accuracy_score(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additional_features(tweet): \n",
    "    \n",
    "    syllables = textstat.syllable_count(tweet)\n",
    "    num_chars = sum(len(w) for w in tweet)\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(tweet.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(tweet.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    add_features=[FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms]\n",
    "    return add_features\n",
    "\n",
    "def get_additonal_feature_array(tweets):\n",
    "    features=[]\n",
    "    for t in tweets:\n",
    "        features.append(additional_features(t))\n",
    "    return np.array(features)\n",
    "\n",
    "fFeatures = get_additonal_feature_array(processed_tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "\n",
    "textstat.syllable_count('tweet')\n",
    "([len(w) for w in 'Hello there man'])\n",
    "# len('Hello there man')\n",
    "###################################\n",
    "fFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_a = tfidf.toarray()\n",
    "modelling_features_enhanced = np.concatenate([tfidf_a,final_features,fFeatures],axis=1)\n",
    "modelling_features_enhanced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#tracking time\n",
    "start = time.time()\n",
    "\n",
    "# Running the model Using TFIDF with additional features from sentiment analysis\n",
    "\n",
    "X = panda.DataFrame(modelling_features_enhanced)\n",
    "y = dataset['class'].astype(int)\n",
    "X_train_features, X_test_features, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, verbose=3).fit(X_train_features,y_train)\n",
    "y_preds = model.predict(X_test_features)\n",
    "report = classification_report( y_test, y_preds )\n",
    "print(report)\n",
    "\n",
    "print(\"Accuracy Score:\" , accuracy_score(y_test,y_preds))\n",
    "\n",
    "end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"time taken: {(end-start)/60}min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix for TFIDF with additional features \n",
    "confusion_matrix = confusion_matrix(y_test,y_preds)\n",
    "matrix_proportions = np.zeros((3,3))\n",
    "for i in range(0,3):\n",
    "    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\n",
    "names=['Hate','Offensive','Neither']\n",
    "confusion_df = panda.DataFrame(matrix_proportions, index=names,columns=names)\n",
    "plt.figure(figsize=(5,5))\n",
    "seaborn.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='YlGnBu',cbar=False, square=True,fmt='.2f')\n",
    "plt.ylabel(r'True Value',fontsize=14)\n",
    "plt.xlabel(r'Predicted Value',fontsize=14)\n",
    "plt.tick_params(labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tweet=''):\n",
    "    if(tweet==''):\n",
    "        new_tweet = str(input(\"Enter a sentence/tweet: \\n\"))\n",
    "    else:\n",
    "        new_tweet = tweet\n",
    "    nt_df = panda.DataFrame({'tweet': new_tweet}, index=range(1))\n",
    "    new_tweet = preprocess(nt_df.tweet)[0]\n",
    "    nt_final_features = np.array(sentiment_analysis(new_tweet)).reshape(1, -1)\n",
    "    nt_tfidf = tfidf_vectorizer.transform([new_tweet]).toarray()\n",
    "    nt_modelling_features = np.concatenate([nt_tfidf, nt_final_features], axis=1)\n",
    "#     print(f\"{nt_tfidf.shape}\\n{nt_final_features.shape}\\n{nt_modelling_features.shape}\")\n",
    "    nt_pred = model_acc.predict(nt_modelling_features)[0]\n",
    "#     return nt_pred[0]\n",
    "    nt_class = ''\n",
    "    if(nt_pred==0): nt_class=\"Extreme racial/offensive hate\"\n",
    "    elif(nt_pred==1): nt_class=\"Non-Extreme hate speech\"\n",
    "    else: nt_class=\"Non-hateful speech\"\n",
    "    print(f\"\\n\\npredicted class: {nt_pred}\\nThe model classifies the given text as: \\\"{nt_class}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
